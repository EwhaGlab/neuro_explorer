{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 18:44:52.881210: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "# orig code is available  @  file:///home/hankm/python_ws/data-science/cnn/CnnEdgeDetection-Keras-Part1.html\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras import models, layers, losses, activations, regularizers, metrics\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import fnmatch\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if True:\n",
    "    import os\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "metricNames = ['loss', 'mean square error', 'Kullback Leibler divergence']\n",
    "BATCH_SIZE = 32\n",
    "(GMHEIGHT, GMWIDTH) = (1000, 1000)\n",
    "METALEN = 18\n",
    "worlds = ['room', 'corner', 'corridor', 'loop_with_corridor','room_with_corner', 'loop']\n",
    "\n",
    "def train_data_generator( world_idxs, num_rounds):\n",
    "    mapimg_dir = '/home/hankm/data/neuro_ffp/train/mapimgs'\n",
    "    gt_dir = '/home/hankm/data/neuro_ffp/train/gt'\n",
    "    metadata_dir = '/home/hankm/data/neuro_ffp/train/metadata'\n",
    "    for widx in world_idxs:\n",
    "        for ridx in range(0, num_rounds) :#99):\n",
    "            curr_mapimg_dir = '%s/%s/round%04d' % (mapimg_dir, worlds[widx], ridx)\n",
    "            curr_gt_dir = '%s/%s/round%04d' % (gt_dir, worlds[widx], ridx)\n",
    "            curr_meta_dir = '%s/%s/round%04d' %(metadata_dir, worlds[widx], ridx)\n",
    "            # count # of images\n",
    "            num_mapimgs = len(fnmatch.filter(os.listdir(curr_mapimg_dir), 'mapimg*'))\n",
    "            for ii in range(0, num_mapimgs):\n",
    "                imgfile = '%s/mapimg%04d.png' % (curr_mapimg_dir, ii)\n",
    "                gtfile  = '%s/gtmap%04d.png' % (curr_gt_dir, ii) # need to be fixed later\n",
    "                metafile = '%s/metadata%04d.txt' % (curr_meta_dir, ii)\n",
    "                mapimg_tmp = cv2.imread(imgfile, 0)/255.\n",
    "                gtfrimg_tmp = cv2.imread(gtfile, 0)/255. # Non-FR (0), FR (1)\n",
    "                # padding to make them 1024 * 1024\n",
    "                mapimg = np.full((1024, 1024), 0.5, dtype=np.float32)\n",
    "                mapimg[12:-12, 12:-12] = mapimg_tmp\n",
    "                mapimg = mapimg[np.newaxis, :, :, np.newaxis]\n",
    "                gtfrimg = np.full((1024, 1024), 0, dtype=np.float32)\n",
    "                gtfrimg[12:-12, 12:-12] = gtfrimg_tmp\n",
    "                gtfrimg = gtfrimg[np.newaxis, :, :, np.newaxis]\n",
    "                #metadata = loadtxt(metafile).astype('float32')\n",
    "                yield mapimg, gtfrimg #, metadata)\n",
    "\n",
    "def val_data_generator( widx, num_rounds):\n",
    "    mapimg_dir = '/home/hankm/data/neuro_ffp/train/mapimgs'\n",
    "    gt_dir = '/home/hankm/data/neuro_ffp/train/gt'\n",
    "    metadata_dir = '/home/hankm/data/neuro_ffp/train/metadata'\n",
    "    for ridx in range(0, num_rounds) :#99):\n",
    "        curr_mapimg_dir = '%s/%s/round%04d' % (mapimg_dir, worlds[widx], ridx)\n",
    "        curr_gt_dir = '%s/%s/round%04d' % (gt_dir, worlds[widx], ridx)\n",
    "        curr_meta_dir = '%s/%s/round%04d' %(metadata_dir, worlds[widx], ridx)\n",
    "        # count # of images\n",
    "        num_mapimgs = len(fnmatch.filter(os.listdir(curr_mapimg_dir), 'mapimg*'))\n",
    "        for ii in range(0, num_mapimgs):\n",
    "            imgfile = '%s/mapimg%04d.png' % (curr_mapimg_dir, ii)\n",
    "            gtfile  = '%s/gtmap%04d.png' % (curr_gt_dir, ii) # need to be fixed later\n",
    "            metafile = '%s/metadata%04d.txt' % (curr_meta_dir, ii)\n",
    "            mapimg_tmp = cv2.imread(imgfile, 0)/255.\n",
    "            gtfrimg_tmp = cv2.imread(gtfile, 0)/255. # Non-FR (0), FR (1)\n",
    "            # padding to make them 1024 * 1024\n",
    "            mapimg = np.full((1024, 1024), 0.5, dtype=np.float32)\n",
    "            mapimg[12:-12, 12:-12] = mapimg_tmp\n",
    "            mapimg = mapimg[np.newaxis, :, :, np.newaxis]\n",
    "            gtfrimg = np.full((1024, 1024), 0, dtype=np.float32)\n",
    "            gtfrimg[12:-12, 12:-12] = gtfrimg_tmp\n",
    "            gtfrimg = gtfrimg[np.newaxis, :, :, np.newaxis]\n",
    "            #metadata = loadtxt(metafile).astype('float32')\n",
    "            yield mapimg, gtfrimg #, metadata)\n",
    "\n",
    "\n",
    "def world2grid(ox, oy, res, wx, wy):\n",
    "    gx = (wx - ox) / res\n",
    "    gy = (wy - oy) / res\n",
    "    return int(gx), int(gy)\n",
    "\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size=3):\n",
    "    '''\n",
    "    Add 2 convolutional layers with the parameters\n",
    "    '''\n",
    "    x = input_tensor\n",
    "    for i in range(2):\n",
    "        x = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=kernel_size,\n",
    "                                   kernel_initializer='he_normal', activation='relu', padding='same')(x)\n",
    "    return x\n",
    "def encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3):\n",
    "    '''\n",
    "    Add 2 convolutional blocks and then perform down sampling on output of convolutions\n",
    "    '''\n",
    "    f = conv2d_block(inputs, n_filters)\n",
    "    p = tf.keras.layers.MaxPooling2D(pool_size=pool_size)(f)\n",
    "    p = tf.keras.layers.Dropout(dropout)(p)\n",
    "    return f, p\n",
    "def encoder(inputs):\n",
    "    '''\n",
    "    defines the encoder or downsampling path.\n",
    "    '''\n",
    "    f1, p1 = encoder_block(inputs, n_filters=64)\n",
    "    f2, p2 = encoder_block(p1, n_filters=128)\n",
    "    f3, p3 = encoder_block(p2, n_filters=256)\n",
    "    f4, p4 = encoder_block(p3, n_filters=512)\n",
    "    return p4, (f1, f2, f3, f4)\n",
    "# Bottlenect\n",
    "def bottleneck(inputs):\n",
    "    bottle_neck = conv2d_block(inputs, n_filters=1024)\n",
    "    return bottle_neck\n",
    "# Decoder\n",
    "def decoder_block(inputs, conv_output, n_filters=64, kernel_size=3, strides=3, dropout=0.3):\n",
    "    '''\n",
    "    defines the one decoder block of the UNet\n",
    "    '''\n",
    "    u = tf.keras.layers.Conv2DTranspose(n_filters, kernel_size, strides, padding='same')(inputs)\n",
    "    c = tf.keras.layers.concatenate([u, conv_output])\n",
    "    c = tf.keras.layers.Dropout(dropout)(c)\n",
    "    c = conv2d_block(c, n_filters)\n",
    "    return c\n",
    "\n",
    "OUTPUT_CHANNELS = 1 # Non-FR or FR\n",
    "def decoder(inputs, convs, output_channels):\n",
    "    '''\n",
    "    Defines the decoder of the UNet chaining together 4 decoder blocks.\n",
    "    '''\n",
    "    f1, f2, f3, f4 = convs\n",
    "    c6 = decoder_block(inputs, f4, n_filters=512, kernel_size=3, strides=2)\n",
    "    c7 = decoder_block(c6, f3, n_filters=256, kernel_size=3, strides=2)\n",
    "    c8 = decoder_block(c7, f2, n_filters=128, kernel_size=3, strides=2)\n",
    "    c9 = decoder_block(c8, f1, n_filters=64, kernel_size=3, strides=2)\n",
    "    if output_channels == 1: # Binary (very important !!!)\n",
    "       activation = 'sigmoid' # Make sure to use sigmoid !!!\n",
    "    else:\n",
    "       activation = 'softmax'\n",
    "    outputs = tf.keras.layers.Conv2D(output_channels, 1, activation=activation)(c9)\n",
    "    return outputs\n",
    "\n",
    "def UNet():\n",
    "    '''\n",
    "    Defines the UNet by connecting the encoder, bottleneck and decoder\n",
    "    '''\n",
    "    inputs = tf.keras.layers.Input(shape=(1024,1024,1))\n",
    "    encoder_output, convs = encoder(inputs)\n",
    "    bottle_neck = bottleneck(encoder_output)\n",
    "    outputs = decoder(bottle_neck, convs, OUTPUT_CHANNELS)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def main(argv):\n",
    "\n",
    "    num_rounds = 10\n",
    "    numdata_worlds = {worlds[0]:0,worlds[1]:0,worlds[2]:0,worlds[3]:0,worlds[4]:0,worlds[5]:0}\n",
    "    # count the total # of samples in each worlds\n",
    "    mapimg_dir = '/home/hankm/data/neuro_ffp/train/mapimgs'\n",
    "\n",
    "    for widx in range(0, len(worlds)):\n",
    "        num_mapimgs = 0\n",
    "        for ridx in range(0, num_rounds):\n",
    "            curr_mapimg_dir = '%s/%s/round%04d' % (mapimg_dir, worlds[widx], ridx)\n",
    "            # count # of images\n",
    "            num = len(fnmatch.filter(os.listdir(curr_mapimg_dir), 'mapimg*'))\n",
    "            num_mapimgs = num_mapimgs + num\n",
    "        numdata_worlds[worlds[widx]] = num_mapimgs\n",
    "\n",
    "    # Define per-fold score containers\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "\n",
    "    for foldidx in range(0,len(worlds)):\n",
    "        worlds_idx = [0,1,2,3,4,5]\n",
    "        worlds_idx.pop(foldidx)\n",
    "        train_ds = tf.data.Dataset.from_generator(lambda: train_data_generator(world_idxs=worlds_idx, num_rounds=num_rounds), output_types=(tf.float32, tf.float32),\n",
    "                                            output_shapes=([1, 1024, 1024, 1], [1, 1024, 1024, 1]))\n",
    "        val_ds   = tf.data.Dataset.from_generator(lambda: val_data_generator(widx=foldidx, num_rounds=num_rounds), output_types=(tf.float32, tf.float32),\n",
    "                                            output_shapes=([1, 1024, 1024, 1], [1, 1024, 1024, 1]))\n",
    "        # model arch\n",
    "        fd_model = UNet()\n",
    "        fd_model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy', #tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {foldidx} ...')\n",
    "        fr_hist = fd_model.fit(train_ds,\n",
    "                               epochs=10,\n",
    "                               verbose=1)\n",
    "        scores = fd_model.evaluate(val_ds, verbose=1)\n",
    "        print(f'Score for fold {foldidx}: {fd_model.metrics_names[0]} of {scores[0]}; {fd_model.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "        #draw history\n",
    "        plt.plot(fr_hist.history['accuracy'])\n",
    "        plt.title('accuracy')\n",
    "        plt.xlabel('accuracy')\n",
    "        plt.ylabel('epoch')\n",
    "        plt.legend(['train','test'], loc='upper left')\n",
    "        outfigfile = '/home/hankm/results/neuro_ffp/fr_res/val_%s_acc.png' % worlds[foldidx]\n",
    "        plt.savefig(outfigfile)\n",
    "        #plt.show()\n",
    "        plt.plot(fr_hist.history['loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        outfigfile = '/home/hankm/results/neuro_ffp/fr_res/val_%s_loss.png' % worlds[foldidx]\n",
    "        #plt.show()\n",
    "        outmodelfile = '/home/hankm/results/neuro_ffp/fr_res/crossval_%s_model.h5'\n",
    "        #fd_model.save(outmodelfile)\n",
    "        outaccfile = '/home/hankm/results/neuro_ffp/fr_res/acc_per_fold.dat'\n",
    "        np_acc_per_fold = np.array(acc_per_fold)\n",
    "        np.savetxt(outaccfile, np_acc_per_fold, fmt='%f', delimiter=' ')\n",
    "        outlosfile = '/home/hankm/results/neuro_ffp/fr_res/loss_per_fold.dat'\n",
    "        np_loss_per_fold = np.array(loss_per_fold)\n",
    "        np.savetxt(outlosfile, np_acc_per_fold, fmt='%f', delimiter=' ')\n",
    "\n",
    "        # == Provide average scores ==\n",
    "        # print('------------------------------------------------------------------------')\n",
    "        # print('Score per fold')\n",
    "        # for i in range(0, len(acc_per_fold)):\n",
    "        #   print('------------------------------------------------------------------------')\n",
    "        #   print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "        # print('------------------------------------------------------------------------')\n",
    "        # print('Average scores for all folds:')\n",
    "        # print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "        # print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "        # print('------------------------------------------------------------------------')\n",
    "        outaccfile = '/home/hankm/results/neuro_ffp/fr_res/acc_per_fold.dat'\n",
    "        np_acc_per_fold = np.array(acc_per_fold)\n",
    "        np.savetxt(outaccfile, np_acc_per_fold, fmt='%f', delimiter=' ')\n",
    "        outlosfile = '/home/hankm/results/neuro_ffp/fr_res/loss_per_fold.dat'\n",
    "        np_loss_per_fold = np.array(loss_per_fold)\n",
    "        np.savetxt(outlosfile, np_acc_per_fold, fmt='%f', delimiter=' ')\n",
    "\n",
    "    print(\"done script the model\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
